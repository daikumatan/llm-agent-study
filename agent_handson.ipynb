{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI エージェントの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, List, Union\n",
    "\n",
    "# .envファイルをロード\n",
    "load_dotenv()\n",
    "\n",
    "# 環境変数OPENAI_API_KEYを取得\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# APIキーが設定されているか確認\n",
    "if openai.api_key is None:\n",
    "    raise ValueError(\"OPENAI_API_KEYが設定されていません。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. AIエージェントのAIエージェントの基本的な概念\n",
    "\n",
    "AIエージェントを理解するためには、その構成要素となる以下の要素を理解する必要があります\n",
    "\n",
    "1. 言語モデル\n",
    "2. プロンプト\n",
    "3. チェーン\n",
    "4. エージェント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. 言語モデル\n",
    "\n",
    "LangChainでOpenAIの言語モデルをインスタンス化する基本的なコードは以下の通りです\n",
    "\n",
    "- OpenAIは、gpt-3.5-turboやgpt-4といった高性能な言語モデルを提供しており、これらは様々な自然言語処理タスクに活用されています\n",
    "- LangChainは、これらの異なる言語モデルを統一的なインターフェースで利用できる抽象化レイヤーを提供しています\n",
    "- これにより、開発者は特定のモデルに依存することなく、様々なモデルを試したり、必要に応じて切り替えたりすることが容易になります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.llms import OpenAI\n",
    "#from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# OpenAIの言語モデルのインスタンスを作成\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. LangChainにおけるPromptTemplate\n",
    "\n",
    "以下のコードは、`PromptTemplate`を使って、特定のトピックに関するジョークを求めるプロンプトを作成する例です。\n",
    "\n",
    "- LangChainは、PromptTemplateという便利なクラスを提供しています\n",
    "- これを利用することで、プレースホルダーを含む動的なプロンプトを作成できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a joke about 人工知能\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# プロンプトのテンプレートを作成。{topic}はプレースホルダー\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "# プレースホルダーに具体的な値を指定してプロンプトを生成\n",
    "prompt = prompt_template.format(topic=\"人工知能\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. `RunnableSequence` を使って処理のChainを作成する\n",
    "\n",
    "以下のコードは、`RunnableSequence`を使ってChainを作成し、実行する例です。\n",
    "\n",
    "- Chainとは、言語モデルへの呼び出しや他のユーティリティを繋ぎ合わせた一連の処理のことです。\n",
    "- LangChainにおける最も基本的なChainは、プロンプトテンプレートと言語モデルを組み合わせたものです。\n",
    "- `RunnableSequence`とパイプ演算子 (|) を使用することで、同様の機能を実現できます。これにより、定義されたプロンプトに基づいて言語モデルを呼び出し、その出力を得ることができます\n",
    "- Chainの概念は、より複雑な処理フローを構築するための基盤となります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Why did the astronaut break up with his girlfriend before going on a space mission? \\n\\nBecause he needed space!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 20, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BbNJG9isPTFzn156FZL80Wafm0bQS', 'finish_reason': 'stop', 'logprobs': None} id='run-25f8ebd7-506a-495f-af16-80af5643c519-0' usage_metadata={'input_tokens': 20, 'output_tokens': 22, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# OpenAIのチャットモデルのインスタンスを作成\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# プロンプトのテンプレートを作成。{topic}はプレースホルダー\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "#\n",
    "# RunnableSequenceを利用することで、複数のステップからなる処理を簡潔に記述し実行することができます\n",
    "#\n",
    "\n",
    "# RunnableSequenceを使ってチェーンを定義\n",
    "# ここでは、事前に作成した言語モデルのインスタンスllmとプロンプトテンプレートprompt_templateをパイプ演算子 (|) で繋ぎ、チェーンを定義しています\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# チェーンを実行し、結果を取得\n",
    "# そして、invokeメソッドに具体的な入力（ここでは「宇宙探査」）を辞書形式で渡すことで、プロンプトが生成され、言語モデルがそのプロンプトに基づいて応答を生成します。\n",
    "output = chain.invoke({\"topic\": \"宇宙探査\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. エージェント\n",
    "\n",
    "LangChainにおけるAIエージェントは、言語モデルを利用して、どのような行動を取るべきかを決定するエンティティです。\n",
    "\n",
    "- 多くの場合、エージェントはツールと呼ばれる外部の機能を利用して、現実世界とインタラクションしたり、追加の情報を取得したりします\n",
    "- AIエージェントの核となるのは、言語モデル、利用可能なツールのセット、そしてどのツールをいつ使用するかを決定するロジック（エージェントロジック）です\n",
    "- LangChainのエージェントは、単に言語モデルに問い合わせるだけでなく、自律的に意思決定を行い、タスクを実行する能力を持つ点が特徴です"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 簡単な質問応答を行うAIエージェントの構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. 最もシンプルなAIエージェントの例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、LangChainとOpenAIを用いて、簡単な質問応答を行うAIエージェントのサンプルコードを作成します。  \n",
    "まず、質問を入力として受け取り、それに対する回答を生成する基本的なエージェントを`RunnableSequence`を用いて実装します。\n",
    "\n",
    "- まず質問応答に特化したプロンプトテンプレートを作成しています\n",
    "- `{question}`は質問の内容が入るプレースホルダーです\n",
    "- 次に、OpenAIのチャットモデルをインスタンス化し、作成したプロンプトテンプレートと組み合わせてRunnableSequenceでチェーンを作成します\n",
    "- 最後に、質問を設定し、invokeメソッドを実行することで、言語モデルが質問に対する回答を生成します\n",
    "\n",
    "このシンプルな例でも、言語モデルが持つ知識を活用して、基本的な質問応答エージェントとして機能することがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "質問: フランスの首都はどこですか？\n",
      "回答: content='パリです。' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 32, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BbNMIbXVnLwkSj2HcDwDjtJGj9VSC', 'finish_reason': 'stop', 'logprobs': None} id='run-b7ea5a71-f3ba-4aa4-85b4-9684505492bd-0' usage_metadata={'input_tokens': 32, 'output_tokens': 4, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 質問応答のためのプロンプトテンプレートを作成\n",
    "qa_prompt_template = PromptTemplate.from_template(\"次の質問に答えてください: {question}\")\n",
    "\n",
    "# OpenAIのチャットモデルをインスタンス化\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# RunnableSequenceを使ってチェーンを定義\n",
    "qa_chain = qa_prompt_template | llm\n",
    "\n",
    "# 質問を設定\n",
    "question = \"フランスの首都はどこですか？\"\n",
    "\n",
    "# チェーンを実行して回答を得る\n",
    "answer = qa_chain.invoke({\"question\": question})\n",
    "\n",
    "# 質問と回答を出力\n",
    "print(f\"質問: {question}\")\n",
    "print(f\"回答: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. コンテキスト（文脈情報）をプロンプトに含めてみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらに、質問に対する回答の精度と関連性を向上させるために、コンテキスト（文脈情報）をプロンプトに含めることができます。  \n",
    "以下のコードは、コンテキストに基づいて質問に答えるようにプロンプトテンプレートを修正した例です。\n",
    "\n",
    "- プロンプトテンプレートに`{context}`というプレースホルダーを追加し、質問に答えるための文脈情報を提供できるようにしています\n",
    "- `invoke`メソッドを実行する際には、プレースホルダーに対応する値を辞書形式で渡します\n",
    "- このように明示的にコンテキストを提供することで、言語モデルはより正確で関連性の高い回答を生成することができます\n",
    "- 特に、特定のドメインや知識に基づいた質問応答を行う場合には、コンテキストの提供が非常に重要になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "質問: フランスの首都はどこですか？\n",
      "文脈: フランスの首都はパリです。\n",
      "回答: content='パリです。' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 66, 'total_tokens': 70, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BbNMamkz7imtJcLjEmPWaTZKshkkr', 'finish_reason': 'stop', 'logprobs': None} id='run-f0261e07-7ac4-49ce-a1a4-6616e52bec84-0' usage_metadata={'input_tokens': 66, 'output_tokens': 4, 'total_tokens': 70, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "qa_with_context_prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"次の文脈に基づいて質問に答えてください。\n",
    "    文脈: {context}\n",
    "    質問: {question}\"\"\"\n",
    ")\n",
    "\n",
    "# OpenAIのチャットモデルをインスタンス化（前の例でインスタンス化されたものを使用）\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# RunnableSequenceを使ってチェーンを定義\n",
    "qa_with_context_chain = qa_with_context_prompt_template | llm\n",
    "\n",
    "# 文脈と質問を設定\n",
    "context = \"フランスの首都はパリです。\"\n",
    "question = \"フランスの首都はどこですか？\"\n",
    "\n",
    "# チェーンを実行して回答を得る。プレースホルダーに対応する値を辞書形式で渡す\n",
    "answer = qa_with_context_chain.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "# 質問、文脈、回答を出力\n",
    "print(f\"質問: {question}\")\n",
    "print(f\"文脈: {context}\")\n",
    "print(f\"回答: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ツールを利用して高度なAIエージェントを構築する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "より高度なAIエージェントは、外部のツールを利用することで、その能力を大幅に拡張できます。  \n",
    "ツールとは、エージェントが外部の世界とインタラクションしたり、追加の情報を取得したりするために使用できる機能のことです。例えば、以下のようなツールが考えられます。\n",
    "\n",
    "- 検索エンジン\n",
    "- 計算機\n",
    "- データベース\n",
    "- API\n",
    "\n",
    "ツールを利用することで、AIエージェントは単なる言語生成だけでなく、より複雑なタスクを実行できるようになります。\n",
    "LangChainでは、Toolクラスを抽象化しており、これを利用することで様々なツールを統一的な方法で定義し、エージェントに利用させることができます。  \n",
    "\n",
    "- Toolは、通常、名前、説明、そして実行するための関数を持ちます\n",
    "- エージェントは、与えられたユーザーのクエリに基づいて、どのツールが適切かを判断します\n",
    "- 選択したツールを実行して、得られた結果を基に最終的な応答を生成します\n",
    "\n",
    "この一連の処理は、エージェントの実行ループと呼ばれます。  \n",
    "エージェントは、ユーザーからのクエリを受け取り、適切なツールを選択し、実行し、その結果を観察し、最終的な応答を生成するというサイクルを繰り返します。\n",
    "\n",
    "ここでは、簡単な例として、簡単な計算（足し算）を行うツールを定義し、それを利用するAIエージェントを構築してみましょう。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. ツールとLLMの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.tools import Tool, StructuredTool\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from pydantic import BaseModel, Field # ★追加: PydanticのBaseModelとFieldをインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydanticを使用して、ツールの引数スキーマを定義します\n",
    "class AddInput(BaseModel):\n",
    "    \"\"\"Inputs for add function.\"\"\"\n",
    "    # 以下のフィールドは、ツールの引数として使用されます。またLLMが適切なToolを選択するためのヒントにもなります\n",
    "    a: float = Field(description=\"The first number to add.\")\n",
    "    b: float = Field(description=\"The second number to add.\")\n",
    "\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"二つの数値を足します。\"\"\"\n",
    "    # 実行ログを追加\n",
    "    print(f\"\\n--- Executing Tool: Simple_Calculator with inputs a={a}, b={b} ---\")\n",
    "    return a + b\n",
    "\n",
    "# StructuredTool を使用します\n",
    "calculator_tool = StructuredTool(\n",
    "    name=\"Simple_Calculator\",\n",
    "    func=add,                           # 先ほど定義した関数を指定\n",
    "    description=\"2つの数値を足します。\",    # ツールの説明\n",
    "    args_schema=AddInput                # 引数スキーマを指定  \n",
    ")\n",
    "\n",
    "tools = [calculator_tool]\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. 状態の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下はAIエージェントの **「現在の状態（おぼえていること）」を定義している**部分\n",
    "\n",
    "- エージェントは、次に何をすべきか判断するために、これまでの会話や途中の結果など、いろいろなことを覚えておく必要がある\n",
    "- この **「おぼえていること」を入れる箱の設計図**が `AgentState` \n",
    "- `operator.add`: LangGraph のシステムに対して、「もし誰か（ノード）が新しいメッセージを返してきたら、その新しいメッセージを、この『会話の記録（messages）』リストの一番最後にそのままつけ加えなさい」というルールを伝えている\n",
    "- `TypedDict`:『現在の状態』という箱には、『messages』という『会話のきろくリスト』が入るよ」という設計図を作っています。\n",
    "- `messages: Sequence[BaseMessage]` : 「会話の記録」 は、メッセージを順番に並べたリストという型を示している\n",
    "- `Annotated[..., operator.add]` :「会話の記録に新しいメッセージをリストの一番最後に付け加えるという特別なルールがあるよ」という指示をLangGraphに与えている\n",
    "  - operator.add がリストにとっての「足し算＝結合」を意味するため、このような動きになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. ノード関数の定義 (変更なし)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(state: AgentState):\n",
    "    \"\"\"現在の状態（メッセージ履歴）に基づいてLLMを呼び出し、応答を生成します。\"\"\"\n",
    "    messages = state['messages']\n",
    "    print(f\"\\n--- Calling LLM with messages: ---\\n{messages}\\n--- End of messages ---\")\n",
    "\n",
    "    # LLMに利用可能なツールを伝えます。\n",
    "    # args_schema を定義したツールを使うことで、LLMは引数の構造を理解しやすくなります。\n",
    "    # ツールの定義をLLMに渡すことで、LLMはどのツールを使うべきかを判断します。複数のツールがある場合、LLMは最も適切なツールを選択します。\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "\n",
    "    print(f\"--- LLM Response: ---\\n{response}\\n--- End of LLM Response ---\")\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. グラフの構築 (変更なし) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10bd295d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# StateGraph をインスタンス化し、エージェントの状態（記憶やデータ）の型として AgentState を指定します。\n",
    "# AgentState で定義された構造（今回は messages リスト）が、グラフの各ノード間を流れ、更新されていきます。\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# グラフにノード（処理のステップ）を追加します。\n",
    "# \"llm\" という名前で、事前に定義した call_llm 関数（LLM呼び出しロジック）を実行するノードを登録します。\n",
    "# グラフがこの \"llm\" ノードに到達すると、call_llm 関数が現在の状態（state）を受け取って実行されます。\n",
    "workflow.add_node(\"llm\", call_llm)\n",
    "\n",
    "# もう一つノードを追加します。\n",
    "# \"action\" という名前で、ToolNode のインスタンスを登録します。\n",
    "# ToolNode は LangGraph に組み込まれた便利なノードで、LLMが生成したツール呼び出しを自動的に解釈し、\n",
    "# コンストラクタに渡されたツールリスト (tools) の中から該当するツールを実行します。\n",
    "# グラフがこの \"action\" ノードに到達すると、前のノード（通常はLLM）からの出力を見てツール実行が必要か判断し、実行します。\n",
    "workflow.add_node(\"action\", ToolNode(tools))\n",
    "\n",
    "# グラフの実行開始地点を設定します。\n",
    "# グラフを起動した際に、最初にどのノードから処理を開始するかを指定します。\n",
    "# この場合、実行は \"llm\" ノードから始まり、LLMがユーザー入力に対して最初の応答や判断を行います。\n",
    "workflow.set_entry_point(\"llm\")\n",
    "\n",
    "# ※ この後、add_conditional_edges や add_edge を使って、\n",
    "#    各ノードの実行後に「次にどのノードへ進むか」という繋がり（エッジ）を定義します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. エッジ（ノード間の遷移）の定義\n",
    "\n",
    "グラフのノード間の遷移を定義します。LLMがツールを呼び出したら 'action'へ、最終応答なら終了、という流れを定義しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 条件付きエッジの追加 (条件分岐を定義できる)\n",
    "\n",
    "これは、特定のノードの実行結果（現在の状態）に基づいて、次に実行するノードを動的に決定するための重要な部分です。\n",
    "\n",
    "- `\"llm\"` \n",
    "\t- この条件分岐の「出発ノード」を指定\n",
    "    - \"llm\" ノードの処理が完了した後に、どのノードへ進むかをこの定義で判断する\n",
    "- `lambda state: \"action\" if state['messages'][-1].tool_calls else END`\n",
    "\t- 現在のエージェントの「状態」(state) を受け取り、次に進むべきノードの名前を指定する。ここでは以下のどちらかが選択される\n",
    "\t\t- \"action\": 足し算を行う\n",
    "\t\t- \"END\": グラフの終了を示す特別な値\n",
    "\n",
    "**lamda関数の詳細**\n",
    "\n",
    "- `state['messages'][-1]`: \n",
    "\t- 現在の状態が持つメッセージリストの中の**一番最後のメッセージ**を取得\n",
    "\t- このケースでは \"llm\" ノードからの出力（LLMの応答）\n",
    "- `.tool_calls`: \n",
    "\t- その「一番最後のメッセージ」（LLMの応答）に、LLM がツールを呼び出すよう指示する情報 **tool_calls オブジェクト**が含まれているかどうかを示す属性\n",
    "\t- ツール呼び出し指示があれば `True` になり、なければ `None` になる\n",
    "- `state['messages'][-1].tool_calls`\n",
    "\t- `True` であれば文字列 \"action\" が返却される\n",
    "\t\t- これにより、グラフは次に \"action\" ノード（ツール実行ノード）へ遷移する\n",
    "\t- その他の場合、例えばLLMが最終的な回答を生成した場合などLangGraph の特殊な値 `END` が返され、グラフの実行は終了する\n",
    "\t\t- `END` は、LangGraph の特別な値で、グラフの実行を終了することを示す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10bd295d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_conditional_edges(\n",
    "    \"llm\", # 出発ノード\n",
    "    lambda state: \"action\" if state['messages'][-1].tool_calls else END, # 条件と次のノード/終了\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**補足**: 上記の2つ目の引数は以下のような関数オブジェクトでもOK, 今回はlambda関数を利用している\n",
    "\n",
    "```python\n",
    "def determine_next_node(state):\n",
    "    \"\"\"\n",
    "    現在の状態を受け取り、LLM応答に基づいて次に進むノードを判断する関数。\n",
    "    LLMの最後のメッセージにtool_callsがあれば'action'を、なければENDを返す。\n",
    "    \"\"\"\n",
    "    # 状態から最後のメッセージを取得\n",
    "    last_message = state['messages'][-1]\n",
    "    # 最後のメッセージに tool_calls 属性があり、かつそれが空でなければ（ツール呼び出しがあれば）\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "         return \"action\" # 'action' ノードに進む\n",
    "    # そうでなければ（最終応答など）\n",
    "    else:\n",
    "        return END # グラフの実行を終了する\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 無条件エッジの追加 (Aノードを処理したら次に必ずXに進んでを定義)\n",
    "\n",
    "特定のノードの実行が完了した後に、**常に次の指定したノードへ遷移**するためのもの。  \n",
    "例えば,Aを処理したら必ずXに進んでねを定義できる\n",
    "\n",
    "- `'action'`: この遷移の「出発ノード」を指定。上記説明のAに相当\n",
    "- `'llm'`: この遷移の「到達ノード」を指定。上記説明のXに相当\n",
    "\n",
    "つまり`action`ノードの実行後は、常に`llm`ノードへ進む\n",
    "\n",
    "- ツール実行ノードはツールを実行し、その結果（ToolMessage）を状態に追加する\n",
    "- そのツール結果を見た上で、LLM に「次に何をすべきか」（さらに別のツールを使うか、それとも最終回答を生成するかなど）を判断させる必要があるから\n",
    "\n",
    "これにより、「LLMで考える → ツールで実行 → 結果を見てLLMでまた考える → ...」という ReAct のようなループ構造が生まれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10bd295d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_edge('action', 'llm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    %% Start node\n",
    "    start_node((\"Start<br/>(workflow.set_entry_point)\"));\n",
    "\n",
    "    %% Entry point: From Start to 'llm' node\n",
    "    start_node --> llm_node;\n",
    "\n",
    "    %% Node definitions and transitions\n",
    "    llm_node[\"Node 'llm'<br/>(call_llm function)\"] --> check_condition{Does the last LLM response<br/>contain tool_calls?};\n",
    "\n",
    "    check_condition --> |\"Yes<br/>(tool_calls exist)\"| action_node[\"Node 'action'<br/>(ToolNode(tools))\"];\n",
    "\n",
    "    check_condition --> |\"No<br/>(no tool_calls)\"| end_node((End));\n",
    "\n",
    "    %% Unconditional transition from 'action' node back to 'llm' node\n",
    "    action_node --> llm_node;\n",
    "\n",
    "    %% End node\n",
    "    end_node((End));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. グラフのコンパイル (変更なし)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StateGraphを使用してワークフローを定義\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. 実行例1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Invoking the agent ---\n",
      "\n",
      "--- Calling LLM with messages: ---\n",
      "[HumanMessage(content='10と25を足してくれますか？', additional_kwargs={}, response_metadata={})]\n",
      "--- End of messages ---\n",
      "--- LLM Response: ---\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_BbOvtCV85eRRqlIrP5znbhIJ', 'function': {'arguments': '{\"a\":10,\"b\":25}', 'name': 'Simple_Calculator'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 83, 'total_tokens': 103, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BbNnQG0wT5e7UhQfotRhSOHXjVXgo', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-b48ba7c9-98ad-48ef-8f1a-6f155039fb63-0' tool_calls=[{'name': 'Simple_Calculator', 'args': {'a': 10, 'b': 25}, 'id': 'call_BbOvtCV85eRRqlIrP5znbhIJ', 'type': 'tool_call'}] usage_metadata={'input_tokens': 83, 'output_tokens': 20, 'total_tokens': 103, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "--- End of LLM Response ---\n",
      "--- Step Output: {'llm': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_BbOvtCV85eRRqlIrP5znbhIJ', 'function': {'arguments': '{\"a\":10,\"b\":25}', 'name': 'Simple_Calculator'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 83, 'total_tokens': 103, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BbNnQG0wT5e7UhQfotRhSOHXjVXgo', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-b48ba7c9-98ad-48ef-8f1a-6f155039fb63-0', tool_calls=[{'name': 'Simple_Calculator', 'args': {'a': 10, 'b': 25}, 'id': 'call_BbOvtCV85eRRqlIrP5znbhIJ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 83, 'output_tokens': 20, 'total_tokens': 103, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}---\n",
      "Node: llm\n",
      "--------------------\n",
      "\n",
      "--- Executing Tool: Simple_Calculator with inputs a=10.0, b=25.0 ---\n",
      "--- Step Output: {'action': {'messages': [ToolMessage(content='35.0', name='Simple_Calculator', tool_call_id='call_BbOvtCV85eRRqlIrP5znbhIJ')]}}---\n",
      "Node: action\n",
      "--------------------\n",
      "\n",
      "--- Calling LLM with messages: ---\n",
      "[HumanMessage(content='10と25を足してくれますか？', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_BbOvtCV85eRRqlIrP5znbhIJ', 'function': {'arguments': '{\"a\":10,\"b\":25}', 'name': 'Simple_Calculator'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 83, 'total_tokens': 103, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BbNnQG0wT5e7UhQfotRhSOHXjVXgo', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-b48ba7c9-98ad-48ef-8f1a-6f155039fb63-0', tool_calls=[{'name': 'Simple_Calculator', 'args': {'a': 10, 'b': 25}, 'id': 'call_BbOvtCV85eRRqlIrP5znbhIJ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 83, 'output_tokens': 20, 'total_tokens': 103, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='35.0', name='Simple_Calculator', tool_call_id='call_BbOvtCV85eRRqlIrP5znbhIJ')]\n",
      "--- End of messages ---\n",
      "--- LLM Response: ---\n",
      "content='10と25を足すと、35になります。' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 116, 'total_tokens': 132, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BbNnRbmJGDITTMBQVClPmvlb37Gpx', 'finish_reason': 'stop', 'logprobs': None} id='run-3516ac89-73ef-4f49-aea6-b5b33049253c-0' usage_metadata={'input_tokens': 116, 'output_tokens': 16, 'total_tokens': 132, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "--- End of LLM Response ---\n",
      "--- Step Output: {'llm': {'messages': [AIMessage(content='10と25を足すと、35になります。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 116, 'total_tokens': 132, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BbNnRbmJGDITTMBQVClPmvlb37Gpx', 'finish_reason': 'stop', 'logprobs': None}, id='run-3516ac89-73ef-4f49-aea6-b5b33049253c-0', usage_metadata={'input_tokens': 116, 'output_tokens': 16, 'total_tokens': 132, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}---\n",
      "Node: llm\n",
      "--------------------\n",
      "--- Agent finished ---\n"
     ]
    }
   ],
   "source": [
    "# エージェントに実行させる入力メッセージ\n",
    "inputs = {\"messages\": [HumanMessage(content=\"10と25を足してくれますか？\")]}\n",
    "\n",
    "print(\"\\n--- Invoking the agent ---\")\n",
    "# 定義したグラフを実行します\n",
    "# verbose=True は invoke 自体の詳細なログは出しません（ノード内のprintは出ます）\n",
    "# stream を使うと、各ノードの出力を逐次確認できます。\n",
    "for step in app.stream(inputs, {\"recursion_limit\": 150}): # recursion_limitはループ回数の上限\n",
    "    print(f\"--- Step Output: {step}---\")\n",
    "    for key, value in step.items():\n",
    "        print(f\"Node: {key}\")\n",
    "        # print(f\"State: {value}\") # 状態全体を表示すると冗長になるためコメントアウト\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "print(\"--- Agent finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8. 実行例2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Invoking the agent (Greeting) ---\n",
      "\n",
      "--- Calling LLM with messages: ---\n",
      "[HumanMessage(content='こんにちは！', additional_kwargs={}, response_metadata={})]\n",
      "--- End of messages ---\n",
      "--- LLM Response: ---\n",
      "content='こんにちは！どのようにお手伝いしましょうか？' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 73, 'total_tokens': 94, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BbNndsWYGwFF9zhgOPd9zMPfJU3YP', 'finish_reason': 'stop', 'logprobs': None} id='run-c77a5013-d9c1-48f2-960b-3cadd5c1cd52-0' usage_metadata={'input_tokens': 73, 'output_tokens': 21, 'total_tokens': 94, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "--- End of LLM Response ---\n",
      "--- Step Output ---\n",
      "Node: llm\n",
      "--------------------\n",
      "--- Agent finished ---\n"
     ]
    }
   ],
   "source": [
    "# 別の実行例（ツールを使わないケース）\n",
    "inputs_2 = {\"messages\": [HumanMessage(content=\"こんにちは！\")]}\n",
    "print(\"\\n--- Invoking the agent (Greeting) ---\")\n",
    "for step in app.stream(inputs_2, {\"recursion_limit\": 150}):\n",
    "     print(\"--- Step Output ---\")\n",
    "     for key, value in step.items():\n",
    "         print(f\"Node: {key}\")\n",
    "         # print(f\"State: {value}\")\n",
    "     print(\"-\" * 20)\n",
    "print(\"--- Agent finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 掛け算も可能にしよう！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydanticを使用して、ツールの引数スキーマを定義します\n",
    "class AddInput(BaseModel):\n",
    "    \"\"\"Inputs for add function.\"\"\"\n",
    "    # 以下のフィールドは、ツールの引数として使用されます。またLLMが適切なToolを選択するためのヒントにもなります\n",
    "    a: float = Field(description=\"The first number to add.\")\n",
    "    b: float = Field(description=\"The second number to add.\")\n",
    "\n",
    "class MultInput(BaseModel):\n",
    "    \"\"\"Inputs for add function.\"\"\"\n",
    "    # 以下のフィールドは、ツールの引数として使用されます。またLLMが適切なToolを選択するためのヒントにもなります\n",
    "    a: float = Field(description=\"The first number to mult.\")\n",
    "    b: float = Field(description=\"The second number to mult.\")\n",
    "\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"二つの数値を足します。\"\"\"\n",
    "    # 実行ログを追加\n",
    "    print(f\"\\n--- {a} + {b} ---\")\n",
    "    return a + b\n",
    "\n",
    "def mult(a: float, b: float) -> float:\n",
    "    \"\"\"二つの数値を掛け合わせます。\"\"\"\n",
    "    # 実行ログを追加\n",
    "    print(f\"\\n--- {a} * {b} ---\")\n",
    "    return a * b\n",
    "\n",
    "# StructuredTool を使用します\n",
    "calculator_tool_add = StructuredTool(\n",
    "    name=\"Simple_Calculator_add\",\n",
    "    func=add,                           # 先ほど定義した関数を指定\n",
    "    description=\"2つの数値を足します。\",    # ツールの説明\n",
    "    args_schema=AddInput               # 引数スキーマを指定  \n",
    ")\n",
    "\n",
    "calculator_tool_mult = StructuredTool(\n",
    "    name=\"Simple_Calculator_mult\",\n",
    "    func=mult,                           # 先ほど定義した関数を指定\n",
    "    description=\"2つの数値を乗算します。\",    # ツールの説明\n",
    "    args_schema=MultInput                 # 引数スキーマを指定  \n",
    ")\n",
    "\n",
    "tools = [calculator_tool_add, calculator_tool_mult]\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(state: AgentState):\n",
    "    \"\"\"現在の状態（メッセージ履歴）に基づいてLLMを呼び出し、応答を生成します。\"\"\"\n",
    "    messages = state['messages']\n",
    "    print(f\"\\n--- Calling LLM with messages: ---\\n{messages}\\n--- End of messages ---\")\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    print(f\"--- LLM Response: ---\\n{response}\\n--- End of LLM Response ---\")\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"llm\", call_llm)\n",
    "workflow.add_node(\"action\", ToolNode(tools))\n",
    "workflow.set_entry_point(\"llm\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"llm\", # 出発ノード\n",
    "    lambda state: \"action\" if state['messages'][-1].tool_calls else END, # 条件と次のノード/終了\n",
    ")\n",
    "workflow.add_edge('action', 'llm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StateGraphを使用してワークフローを定義\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エージェントに実行させる入力メッセージ\n",
    "#inputs = {\"messages\": [HumanMessage(content=\"10と25を足してください\")]}\n",
    "inputs = {\"messages\": [HumanMessage(content=\"10に25を掛けてください\")]}\n",
    "\n",
    "print(\"\\n--- Invoking the agent ---\")\n",
    "# 定義したグラフを実行します\n",
    "# verbose=True は invoke 自体の詳細なログは出しません（ノード内のprintは出ます）\n",
    "# stream を使うと、各ノードの出力を逐次確認できます。\n",
    "for step in app.stream(inputs, {\"recursion_limit\": 150}): # recursion_limitはループ回数の上限\n",
    "    print(f\"--- Step Output: {step}---\")\n",
    "    for key, value in step.items():\n",
    "        print(f\"Node: {key}\")\n",
    "        # print(f\"State: {value}\") # 状態全体を表示すると冗長になるためコメントアウト\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "print(\"--- Agent finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: 理解を助けるためのサンプルコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMがどのように適正に使用するツールを選択するのか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM が登録されたツールを適切に選択し、そして正しく呼び出すために最も重要な情報は、実は**複数あり、それぞれが異なる役割を果たします。**\n",
    "\n",
    "ご質問の `description=\"2つの数値を足します。\"` は、確かに**非常に重要な情報の一つ**です。これは LLM に対して、そのツールが「何を達成するためのものか」を自然言語で伝える、ツールの**目的の要約**だからです。LLM はユーザーの要求とツールの説明を比較して、「ユーザーの『足してほしい』という要求には、この『2つの数値を足す』ツールが合っているな」と判断します。\n",
    "\n",
    "しかし、目的が分かっただけでは、LLM はツールを**正しく呼び出す**ことができません。ツールを呼び出すには、**何という名前で呼び出すか**、そして**どのような情報を引数として渡すか**を知る必要があります。\n",
    "\n",
    "ここで重要になるのが：\n",
    "\n",
    "1.  **ツールの名前 (`name=\"Simple_Calculator\"`)**: LLM が「このツールを使おう」と決めたときに、その判断をシステムに伝えるための固有の識別子です。\n",
    "2.  **`args_schema` (Pydantic モデル `AddInput` とその中の `Field` 定義)**: これが、ツールが**「どんな種類の情報（引数）を、どのような名前で、いくつ必要としているか」**を伝える構造化された情報です。\n",
    "    * `args_schema` に含まれる **引数名** (`a`, `b`)：LLM はこれで、「このツールには `a` と `b` という名前の情報が必要なんだな」と理解します。\n",
    "    * `args_schema` に含まれる **引数の型** (`float`)：LLM はこれで、「`a` と `b` には数値（小数点を含む可能性のある数）を渡せばいいんだな」と理解します。\n",
    "    * `args_schema` の `Field` に含まれる **引数の説明** (`description=\"The first number to add.\"` など)：これは**非常に重要です。**LLM はこの説明を見て、「`a` というのは『足し算する一つ目の数』のことか」「`b` は『二つ目の数』のことか」と理解し、ユーザーのプロンプトの中からどの部分を `a` に、どの部分を `b` に当てはめれば良いかを判断します。\n",
    "\n",
    "**結論として：**\n",
    "\n",
    "* ツール全体の **`description`** は、LLM が「このツールは関連しているか？」と判断するための**入口（目的の理解）**として非常に重要です。\n",
    "* `args_schema` 全体（特に**引数名**と**引数の `description`**）は、LLM が「このツールを使うためには何が必要か？」「ユーザーの要求から、必要な情報はどこにあるか？」「どのような形式で情報を渡せば良いか？」と判断するための**具体的な手順書**として非常に重要です。\n",
    "\n",
    "どちらか一方だけでは不十分です。たとえるなら、\n",
    "\n",
    "* 全体の `description` は「これは料理を作る道具です（包丁です）」という説明です。\n",
    "* `args_schema` は「材料を切るには、『切るもの（野菜）』と『切り方（薄切り/みじん切りなど）』が必要です」という具体的な使い方と必要な情報の説明です。\n",
    "\n",
    "LLM がツールを適切に選択し、そしてエラーなく実行させるためには、**ツール全体の `description`** と、`args_schema` 内の**各引数の `description`** の両方が、**明確で正確であること**が最も重要と言えます。どちらも欠かせない要素です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TypeDictの理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "# 「ユーザー情報」という辞書のレシピ\n",
    "class UserInfo(TypedDict):\n",
    "    name: str   # 'name'というキーがあり、値は文字列(str)\n",
    "    age: int    # 'age'というキーがあり、値は整数(int)\n",
    "    is_active: bool # 'is_active'というキーがあり、値は真偽値(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正常系のテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このレシピに沿った辞書\n",
    "user1: UserInfo = {\"name\": \"山田\", \"age\": 30, \"is_active\": True}\n",
    "user1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 異常系のテスト\n",
    "\n",
    "ここではメッセージはでない.MyPy等を使って実行すると警告を得ることができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# レシピに合わない辞書 (ageが文字列なので、型ヒントとしてはエラー)\n",
    "user2: UserInfo = {\"name\": \"田中\", \"age\": \"25\", \"is_active\": False} # 型チェッカーは警告を出す\n",
    "print(user2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pydanticで確実にエラーを出させる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なものをPydanticからインポート\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "# TypedDict の代わりに BaseModel を継承する ★変更点★\n",
    "# クラス名は TypedDict 版と区別するために UserInfoPydantic とします\n",
    "class UserInfoPydantic(BaseModel):\n",
    "    \"\"\"ユーザー情報のPydanticモデル\"\"\"\n",
    "    # フィールド名、型ヒント、Fieldを使った設定を記述\n",
    "    name: str = Field(description=\"ユーザーの名前\")\n",
    "    age: int = Field(description=\"ユーザーの年齢\")\n",
    "    is_active: bool = Field(description=\"ユーザーがアクティブかどうか\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正常系の動作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pydanticモデルの使い方（ランタイム検証のデモ） ---\n",
    "\n",
    "print(\"--- 正しいデータでモデルを作成 ---\")\n",
    "try:\n",
    "    # 定義に合う正しいデータ（辞書）を渡してモデルのインスタンスを作成\n",
    "    user1_data = {\"name\": \"山田\", \"age\": 30, \"is_active\": True}\n",
    "    user1_instance = UserInfoPydantic(**user1_data) # 辞書を展開して渡す\n",
    "    # または user1_instance = UserInfoPydantic.model_validate(user1_data) # こちらの書き方もある\n",
    "\n",
    "    print(\"モデル作成成功:\")\n",
    "    print(user1_instance)\n",
    "    print(f\"名前: {user1_instance.name}, 年齢: {user1_instance.age}\") # インスタンスから属性としてアクセス可能\n",
    "\n",
    "except ValidationError as e:\n",
    "    print(\"モデル作成に失敗しました (予期しないエラー):\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 入力の型は間違っているがPydanticの自動補正機能が働いたケース"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 間違ったデータでモデルを作成（エラー発生のデモ） ---\")\n",
    "try:\n",
    "    # 定義に合わない間違ったデータ（ageが文字列）\n",
    "    user2_data_invalid = {\"name\": \"田中\", \"age\": \"25\", \"is_active\": False}\n",
    "\n",
    "    # 間違ったデータを渡してモデルのインスタンスを作成 ★ここでエラーが発生します★\n",
    "    user2_instance = UserInfoPydantic(**user2_data_invalid)\n",
    "\n",
    "    print(\"モデル作成成功 (このメッセージは表示されないはず):\")\n",
    "    print(user2_instance)\n",
    "\n",
    "except ValidationError as e:\n",
    "    # ValidationError が捕捉される\n",
    "    print(\"モデル作成に失敗しました (期待通り):\")\n",
    "    print(e) # どんなエラーが発生したか詳細が表示される\n",
    "\n",
    "print(\"\\n--- 終了 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自動補正が効かないわざとエラーがでるケース"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 間違ったデータでモデルを作成（エラー発生のデモ） ---\n",
    "print(\"\\n--- 間違ったデータでモデルを作成（エラー発生のデモ） ---\")\n",
    "try:\n",
    "    # 定義に合わない間違ったデータ（ageが文字列、かつ整数に変換できないもの） ★ここを変更★\n",
    "    user2_data_invalid = {\"name\": \"田中\", \"age\": \"二十五\", \"is_active\": False} # 例: 整数に変換できない文字列\n",
    "    # または user2_data_invalid = {\"name\": \"田中\", \"age\": [25], \"is_active\": False} # 例: リスト\n",
    "\n",
    "    # 間違ったデータを渡してモデルのインスタンスを作成 ★ここでValidationErrorが発生します★\n",
    "    user2_instance = UserInfoPydantic(**user2_data_invalid)\n",
    "\n",
    "    print(\"モデル作成成功 (このメッセージは表示されないはず):\")\n",
    "    print(user2_instance)\n",
    "\n",
    "except ValidationError as e:\n",
    "    # ValidationError が捕捉される ★今度はここが実行されます★\n",
    "    print(\"モデル作成に失敗しました (期待通り):\")\n",
    "    print(e) # どんなエラーが発生したか詳細が表示される\n",
    "\n",
    "print(\"\\n--- 終了 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `from typing import Annotated` の機能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Annotated` は、「**いつもの型ヒントに、あとから別の情報をくっつける**」ためのものです。\n",
    "\n",
    "たとえるなら、変数につける「これは数字です」「これは単語です」「これはリストです」という**いつものラベル**に、**特別な「付箋（ふせん）」をペタッと貼り付ける**ようなイメージです。\n",
    "\n",
    "図で見てみましょう。\n",
    "\n",
    "まず、普通の型ヒントのラベルです。\n",
    "\n",
    "```terminal\n",
    "+-----------------+\n",
    "|  Variable Box   |\n",
    "+-----------------+\n",
    "|                 |\n",
    "|                 |\n",
    "|   (Contents)    |\n",
    "|                 |\n",
    "+-----------------+\n",
    "     |\n",
    "     |  This is...\n",
    "     v\n",
    "+--------------+\n",
    "| [Type Hint]  |  ← Usual Label (e.g., int, str, List[something])\n",
    "+--------------+\n",
    "```\n",
    "\n",
    "`Annotated` を使うと、この「いつものラベル」に、さらに別の「付箋」情報をくっつけられます。\n",
    "\n",
    "```terminal\n",
    "+-----------------+\n",
    "|  Variable Box   |\n",
    "+-----------------+\n",
    "|                 |\n",
    "|                 |\n",
    "|   (Contents)    |\n",
    "|                 |\n",
    "+-----------------+\n",
    "     |\n",
    "     |  This is...\n",
    "     v\n",
    "+--------------+   +------------------------+\n",
    "| [Type Hint]  | + | ★Special Sticky Note ★ |  ← Annotated allows adding this note\n",
    "+--------------+   |                        |\n",
    "                   |   Any info you want    |\n",
    "                   |      can be written    |\n",
    "                   +------------------------+\n",
    "```\n",
    "\n",
    "コードの `Annotated[Sequence[BaseMessage], operator.add]` の場合、これはこういう意味になります。\n",
    "\n",
    "1.  **いつものラベル**: `Sequence[BaseMessage]` (メッセージのリストです)\n",
    "2.  **特別な付箋**: `operator.add` (リストを「足し合わせる」、つまり結合するという情報)\n",
    "\n",
    "これを変数 `messages` の型ヒントとして使うと...\n",
    "\n",
    "```terminal\n",
    "+-------------------------+\n",
    "|      messages Box       |\n",
    "+-------------------------+\n",
    "|                         |\n",
    "|  [Message 1]            |\n",
    "|  [Message 2]            |\n",
    "|  ...                    |\n",
    "+-------------------------+\n",
    "     |\n",
    "     |  This is...\n",
    "     v\n",
    "+---------------------------+   +-----------------------+\n",
    "| [It's a List of Messages] | + | ★Special Sticky Note★ |  ← Added by Annotated\n",
    "+---------------------------+   |                       |\n",
    "                                |  [Adding Rule]        |  ← The info from operator.add\n",
    "                                +-----------------------+\n",
    "\n",
    "```\n",
    "\n",
    "ここで重要なのは、この「特別な付箋」に書かれた情報は、**Python が普通にコードを実行するときには、ほとんど無視される**ということです。Python は「メッセージのリストだな」ということだけを見て動きます。\n",
    "\n",
    "しかし、**LangGraph のような「この特別な付箋の読み方を知っている」プログラムやライブラリ**は、この付箋を見つけて書かれた情報を利用します。\n",
    "\n",
    "LangGraph は `messages` の型ヒントに `Annotated` と `operator.add` が付いているのを見ると、「なるほど、この `messages` のリストには、『新しい情報が来たら前の情報に足し合わせる』というルールが設定されているんだな」と理解します。そして、ノードが新しいメッセージのリストを返したとき、その「足し合わせるルール」に従って、元々 `messages` に入っていたリストの**後ろに**新しいリストをくっつける（結合する）という処理を実行します。\n",
    "\n",
    "まとめ：\n",
    "\n",
    "* `Annotated` は、変数に付ける「型」というラベルに、**追加の「特別な情報（付箋）」をペタッと貼り付ける**ためのものです。\n",
    "* この付箋の情報は、**標準のPythonは気にしません**。\n",
    "* しかし、**特別なプログラム（LangGraphなど）は、この付箋を読んで、いつもとは違う特別な動きをする**ように作られています。\n",
    "* `Annotated[Type, metadata]` は、「この変数は `Type` という種類だけど、特別な情報として `metadata` も付いているよ」という意味になります。\n",
    "\n",
    "「このデータは〇〇だよ」という情報に、「△△する時にこの情報を参考にしてね」という補足情報や指示を付ける、と考えれば分かりやすいかもしれません。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-agent-study-9WkdKeUo-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
